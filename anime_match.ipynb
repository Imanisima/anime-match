{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "anime-match.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Ph1sk1KD8cYd"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Imanisima/anime-match/blob/master/anime_match.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVLWLFLMnUwC"
      },
      "source": [
        "# Anime Match\n",
        "----\n",
        "A classification model that uses transcipts from Anime to filter out specific themes. \n",
        "\n",
        "## The Problem\n",
        "---\n",
        "What do we consider when choosing anime? Genre, art style, content length, and popularity are typically what we think about.\n",
        "\n",
        "Current recommendation systems that only filter by broad genre tags allow for specific themes to slip through and also close of entire categories for general thematic elements that are assumed. Broad genres, including horror and fantasy, may include several anime that deal with death and supernatural elements. Any individual that wished to not see either of these categories may eliminate the broad genre, where several shows in that genre don’t deal with either theme. \n",
        "\n",
        "Alternatively, there are several instances where an anime may be tagged by a traditionally light- hearted genre but do include darker themes. For example, the anime “Your Lie in April” is included in the romance genre overall but regularly includes themes of death from the main character’s relative passing to one of the main characters passing by the finale. By focusing on a thematic filter, we can add to existing recommendation systems to better the experience of anime enthusiasts, both by reducing the amount of anime with minor mentions of exclusionary themes to slip through due to their overarching genre tags, and by broadening the available recommendations with previously excluded larger genres.\n",
        "\n",
        "## The Big Picture\n",
        "----\n",
        "Although the classification model we are building is Anime, it could also be applied to Manga, TV shows, books, newspapers, and other content. This model could also be used for parental control for children when they are searching the internet and watching Netflix.\n",
        "\n",
        "## Dataset\n",
        "---\n",
        "The datasets uses in this project are raw transcripts from [Kistunekko](https://kitsunekko.net). It contains transcripts from over 2000 anime in 4 languages: English, Japanese, Chinese, and Korean. For the purposes of this project, we will be sticking with English.\n",
        "\n",
        "Transcipts can be found in the /content/transcripts path.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBTlfDzDBhXR"
      },
      "source": [
        "### I. Web Scraper\n",
        "First, we need to build a webscrapper for the kisunekko.net site! Here are our steps:\n",
        "\n",
        "(1) Build webscrapper using BeautifulSoup\n",
        "\n",
        "(2) We will retreive all zip files from each anime listed.\n",
        "\n",
        "(3) Extract all transcripts from each compressed files and lastly,\n",
        "\n",
        "(4) Remove leftover compressed files to save some space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSkXblo2pVQI",
        "outputId": "5088311d-3e13-4424-f69a-a6ea51289e20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# run this to import from google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKBArgmGriIb"
      },
      "source": [
        "import os\n",
        "import requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YorlMf_EqhJ8",
        "outputId": "283f35f7-b610-4781-b867-e500cea27f55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "'''\n",
        "Web-scraper for kistunekko.net\n",
        "'''\n",
        "\n",
        "domain = \"https://kitsunekko.net\"\n",
        "sub_query = \"/dirlist.php?dir=subtitles\"\n",
        "url = domain + sub_query\n",
        "res = requests.get(url)\n",
        "\n",
        "res"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Response [200]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCvK8S3e4r2X"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "soup = BeautifulSoup(res.content, 'html.parser')\n",
        "\n",
        "table_res = soup.find(id='flisttable') # id that points to the transcripts\n",
        "trans_elem = table_res.find_all('a', class_='') # Using the table results, retrieve the rows with links to transcripts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxVQKrNBBdWY"
      },
      "source": [
        "import re\n",
        "\n",
        "''' Strip html tags from text '''\n",
        "def clean_html(raw_html):\n",
        "  strip_tags = re.compile('<.*?>')\n",
        "  clean_text = re.sub(strip_tags, '', raw_html)\n",
        "  return clean_text\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pv2SBFNpCduX"
      },
      "source": [
        "'''\n",
        "Each Anime has a title and a link for download\n",
        "'''\n",
        "anime_list = {}\n",
        "for a_tag in trans_elem:\n",
        "    title_elem = a_tag.find('strong', class_='')\n",
        "    title = clean_html(str(title_elem))\n",
        "    anime_list[title] = a_tag[\"href\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mptd7FPXsIj1",
        "outputId": "e8d38f78-66c3-4f17-da98-2e22db85a6f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# write list of anime to txt file for later use.\n",
        "print(\"Writing to file...\")\n",
        "anime_list_path = \"/content/drive/My Drive/Colab Notebooks/anime_list.txt\"\n",
        "with open(anime_list_path, \"w\") as f:\n",
        "  for anime in anime_list.keys():\n",
        "    f.write(anime)\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "f.close()\n",
        "\n",
        "print(\"Writing complete.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing to file...\n",
            "Writing complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KryUHNRVdm9u"
      },
      "source": [
        "# !pip install pyunpack # uncomment in case of error\n",
        "from pyunpack import Archive\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "'''extract files from zip, rar, and .7zip files'''\n",
        "def decompress_files(trans_path):\n",
        "  trans_folder = os.listdir(trans_path)\n",
        "\n",
        "  for file in trans_folder:\n",
        "      if (\".rar\" in file or \".zip\" in file or \".7z\" in file):\n",
        "\n",
        "        with open(trans_path + file, \"rb\") as f:\n",
        "          try:\n",
        "            Archive(trans_path + file).extractall(trans_path + file)\n",
        "          except: # in case of a bad zip file or magic number error\n",
        "            pass\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWDjT16OcO-X"
      },
      "source": [
        "# !pip install wget # uncomment to install wget in case of error\n",
        "import wget\n",
        "\n",
        "'''\n",
        "Download file from kisunekko\n",
        "\n",
        "Dir: where to store the download\n",
        "URL: link to download the transcripts\n",
        "'''\n",
        "\n",
        "def download_files(url, dir, title):\n",
        "  dir = os.path.expanduser(dir)\n",
        "  folder = os.path.splitext(dir + title)[0]\n",
        "  download_to = folder + \"/\"\n",
        "\n",
        "  if not os.path.exists(folder):\n",
        "      os.makedirs(folder)\n",
        "\n",
        "  if os.path.exists(os.path.splitext(dir + title)[0]):\n",
        "    wget.download(url=url, out=download_to)\n",
        "\n",
        "  decompress_files(download_to)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wCov-yF4BUS",
        "outputId": "659cb454-f77a-4f1c-906f-a9b0facea565",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "''' Uncomment this cell to delete everything in the folder folder and its contents '''\n",
        "\n",
        "# import shutil\n",
        "\n",
        "# shutil.rmtree(\"/content/drive/My Drive/Colab Notebooks/transcripts/\")\n",
        "# print(\"folder removed\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "folder removed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmOJ86UhVYjp",
        "outputId": "6ef31288-a10c-4798-bd9f-ead915c4ac93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "'''Get path to zip files for downloads.'''\n",
        "\n",
        "trans_path = \"/content/drive/My Drive/Colab Notebooks/transcripts/\" # where to store the transcripts\n",
        "\n",
        "print(\"Downloading from Kistunekko.net...\")\n",
        "for zip_link in anime_list:\n",
        "  zip_url = domain + anime_list[zip_link]\n",
        "  zip_res = requests.get(zip_url)\n",
        "\n",
        "  soup = BeautifulSoup(zip_res.content, 'html.parser')\n",
        "\n",
        "  table_res = soup.find(id='flisttable')\n",
        "  trans_elem = table_res.find_all('a', class_='')\n",
        "\n",
        "  for a_tag in trans_elem:\n",
        "    title_elem = a_tag.find('strong', class_='')\n",
        "    trans_title = clean_html(str(title_elem))\n",
        "\n",
        "    download_url = domain + \"/\" + a_tag[\"href\"]\n",
        "\n",
        "    download_files(download_url, trans_path, trans_title)\n",
        "\n",
        "print(\"Download complete.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading from Kistunekko.net...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph1sk1KD8cYd"
      },
      "source": [
        "## II. Random Generator\n",
        "This will be used to randomly select the anime we will train the model on!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So1HAbYpv-JY"
      },
      "source": [
        "import random\n",
        "\n",
        "\n",
        "''' Read txt file into a list. Select 'N' random anime to be used for training.'''\n",
        "def list_random(n):\n",
        "    anime_list = []\n",
        "\n",
        "    with open(anime_list_path, \"r\") as read_file:\n",
        "      anime_list = [line.strip() for line in read_file]\n",
        "\n",
        "    read_file.close()\n",
        "  \n",
        "    random.shuffle(anime_list)\n",
        "    return anime_list[0:n]"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPMZFzdvwnhn",
        "outputId": "3ebd8c61-b961-4e08-f773-7b16f05ca70a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_anime = list_random(10)\n",
        "train_anime"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Rokudenashi Majutsu Koushi to Akashic Records',\n",
              " 'Hime Chen Otogi Chikku Idol Lilpri',\n",
              " 'Shoujo Shuumatsu Ryokou',\n",
              " 'Oh Edo Rocket',\n",
              " 'Kino no Tabi - the Beautiful World',\n",
              " 'Osamu Tezuka Buddha Movie 2',\n",
              " 'Kodomo no Omocha',\n",
              " 'Kaze no Stigma',\n",
              " 'Toaru Majutsu no Index III',\n",
              " 'Hunter X Hunter - OVA']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJmcEm3n8wNt"
      },
      "source": [
        "## III. Clean Transcripts\n",
        "After randomly selecting the anime, we will select 10 episodes from each anime and run it through the parser."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHfHWVc08o4F"
      },
      "source": [
        "# work in progress"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms-VBfkZ3n-B"
      },
      "source": [
        "## IV. Prototype"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gDr7Mdc4Vah"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.manifold import TSNE\n",
        "import spacy\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Lambda"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fjun8NwyK08i"
      },
      "source": [
        "def parseQuadLinesFile(file):\n",
        "    ''' read quartets of lines from a file '''\n",
        "\n",
        "    transcript = []\n",
        "    title = []\n",
        "    episode = []\n",
        "    label = []\n",
        "\n",
        "\n",
        "    with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i % 2:\n",
        "                title.append(line.strip())\n",
        "            elif i % 3:\n",
        "                episode.append(line.strip())\n",
        "            elif i % 4:\n",
        "                label.append(line.strip())\n",
        "            else:\n",
        "                transcript.append(line.strip())\n",
        "    print(sequenceA)\n",
        "    return transcript, title, episode, label"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrtLGaeBK4gS"
      },
      "source": [
        "def buildDataframe():\n",
        "    ''' organize information from the txt file into a dataframe\n",
        "        easier to visualize and seperate conceptually '''\n",
        "\n",
        "    ### Note: As of November 04, we are still building the files as well\n",
        "    ### \"filename.txt\" is a placeholder for the time being\n",
        "    \n",
        "    file_name = \"filename.txt\"\n",
        "    transcript, title, episode, label = parseQuadLinesFile(file_name)\n",
        "    transcript = np.array(transcript)\n",
        "    data_df = pd.DataFrame({'Transcript': transcript, \n",
        "                          'Title of Anime': title,\n",
        "                          'Title of Episode': episode,\n",
        "                          'Human Gold Label': label})\n",
        "    data_df = data_df[['Transcript', 'Title of Anime', 'Title of Episode', 'Human Gold Label']]\n",
        "    data_df"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYTYXGRhK-bA"
      },
      "source": [
        "def normalize_document(doc):\n",
        "    wpt = nltk.WordPunctTokenizer()\n",
        "    stop_words = nltk.corpus.stopwords.words('english')\n",
        "    \n",
        "    # lower case and remove special characters and whitespaces\n",
        "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
        "    doc = doc.lower()\n",
        "    doc = doc.strip()\n",
        "\n",
        "    # tokenize document\n",
        "    tokens = wpt.tokenize(doc)\n",
        "\n",
        "    # filter out stopwords from document\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # re-create document from filtered tokens\n",
        "    doc = ' '.join(filtered_tokens)\n",
        "    return doc"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWbwnHpE4gQk"
      },
      "source": [
        "'''Parse original file'''\n",
        "buildDataframe()\n",
        "\n",
        "''' Normalize document '''\n",
        "normalize_transcript = np.vectorize(normalize_document)\n",
        "norm_transcript = normalize_transcript(transcript)\n",
        "norm_transcript\n",
        "\n",
        "unique_words = list(set([word for sublist in [doc.split() for doc in norm_transcript] for word in sublist]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-X4YC9nQXmo"
      },
      "source": [
        "''' Use GloVe to assess word embeddings '''\n",
        "word_glove_vectors = np.array([nlp(word).vector for word in unique_words])\n",
        "pd.DataFrame(word_glove_vectors, index=unique_words)\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=0, n_iter=5000, perplexity=3)\n",
        "np.set_printoptions(suppress=True)\n",
        "T = tsne.fit_transform(word_glove_vectors)\n",
        "labels = unique_words\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NhffIsQQaOr"
      },
      "source": [
        "''' K-means Clustering based on mean of word embedding to determine overall leaning  \n",
        "    Cluster labels indicates leaning and, on a test file, would output the suggested group to which a given transcript would belong\n",
        "'''\n",
        "\n",
        "doc_glove_vectors = np.array([nlp(str(doc)).vector for doc in norm_transcript])\n",
        "\n",
        "km = KMeans(n_clusters=3, random_state=0)\n",
        "km.fit_transform(doc_glove_vectors)\n",
        "cluster_labels = km.labels_\n",
        "cluster_labels = pd.DataFrame(cluster_labels, columns=['ClusterLabel'])\n",
        "pd.concat([transcript_df, cluster_labels], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DGKGLUIQbMq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}