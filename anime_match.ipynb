{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "anime-match.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "OBTlfDzDBhXR",
        "6AQcJlijeKmX",
        "aAXMTKqRaKW8",
        "AYyWVqWv-2fO",
        "Ph1sk1KD8cYd",
        "xdyA-czlkZ3m",
        "KAJ53U_pmt9b",
        "alql3gUPxJXG",
        "cJmcEm3n8wNt",
        "rd9SvOdDEwC3",
        "4Qq2BRBrE0id",
        "QSTAK6xf1OqO",
        "260T_-rW4WvQ",
        "RVwWVXTu4cOs",
        "B9xn9WAbnm-I",
        "l7vwsFYDO5kL",
        "eshE3mGzlmmO",
        "-N9wQueY3oIU",
        "sjSP4OcNlrDD"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Imanisima/anime-match/blob/prototype/anime_match.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVLWLFLMnUwC"
      },
      "source": [
        "# Anime Match\n",
        "----\n",
        "A classification model that uses transcipts from Anime to filter out specific themes. \n",
        "\n",
        "## The Problem\n",
        "---\n",
        "What do we consider when choosing anime? Genre, art style, content length, and popularity are typically what we think about.\n",
        "\n",
        "Current recommendation systems that only filter by broad genre tags allow for specific themes to slip through and also close of entire categories for general thematic elements that are assumed. Broad genres, including horror and fantasy, may include several anime that deal with death and supernatural elements. Any individual that wished to not see either of these categories may eliminate the broad genre, where several shows in that genre don’t deal with either theme. \n",
        "\n",
        "Alternatively, there are several instances where an anime may be tagged by a traditionally light- hearted genre but do include darker themes. For example, the anime “Your Lie in April” is included in the romance genre overall but regularly includes themes of death from the main character’s relative passing to one of the main characters passing by the finale. By focusing on a thematic filter, we can add to existing recommendation systems to better the experience of anime enthusiasts, both by reducing the amount of anime with minor mentions of exclusionary themes to slip through due to their overarching genre tags, and by broadening the available recommendations with previously excluded larger genres.\n",
        "\n",
        "## The Big Picture\n",
        "----\n",
        "Although the classification model we are building is Anime, it could also be applied to Manga, TV shows, books, newspapers, and other content. This model could also be used for parental control for children when they are searching the internet and watching Netflix.\n",
        "\n",
        "## Dataset\n",
        "---\n",
        "The datasets uses in this project are raw transcripts from [Kistunekko](https://kitsunekko.net). It contains transcripts from over 2000 anime in 4 languages: English, Japanese, Chinese, and Korean. For the purposes of this project, we will be sticking with English.\n",
        "\n",
        "Transcipts can be found in the [/content/drive/My Drive/Colab Notebooks/transcripts]() path.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y35ZH3i9Tdng"
      },
      "source": [
        "---\n",
        "## ```0. Environment Set Up```\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSkXblo2pVQI",
        "outputId": "28482369-32ab-4388-b7e9-ed12cc07699e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "''' \n",
        "run this after restarting runtime to mount google drive directories\n",
        "'''\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm6V9C8CL5_A",
        "outputId": "1aa9bd16-4cdd-4a20-8556-45427b51d346",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "'''\n",
        "Install the following libraries\n",
        "\n",
        "'''\n",
        "# print(\"installing packages...\")\n",
        "# !pip install wget\n",
        "# !pip install pyunpack\n",
        "# !pip install scipy\n",
        "# !pip install pysub-parser\n",
        "# !pip install scikit-multilearn\n",
        "# print(\"\\n done!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nInstall the following libraries\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drZ0F3pgTuTO",
        "outputId": "6fcc8920-a74e-44e8-fcd1-71b3ea2d69d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "'''\n",
        "Import the following libraries:\n",
        "'''\n",
        "print(\"importing libraries...\")\n",
        "import os\n",
        "import sys\n",
        "import ntpath\n",
        "import subprocess\n",
        "import zipfile\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import wget\n",
        "\n",
        "from pysubparser.cleaners import ascii, brackets, formatting, lower_case\n",
        "from pysubparser import parser\n",
        "\n",
        "import re\n",
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pyunpack import Archive\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from scipy import spatial\n",
        "print(\"\\n done!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "importing libraries...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "\n",
            " done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrxoMsMkN_Xb"
      },
      "source": [
        "'''\n",
        "List of paths needed for reusability\n",
        "'''\n",
        "\n",
        "notebook_path = \"/content/drive/My Drive/Colab Notebooks\"\n",
        "\n",
        "### Transcripts\n",
        "anime_list_path = f\"{notebook_path}/anime_list.txt\" # list of anime titles\n",
        "trans_path = f\"{notebook_path}/transcripts/\" # where to store the transcripts\n",
        "\n",
        "### Training and Testing Data\n",
        "train_path = f\"{notebook_path}/train_texts/\" # store parsed training data\n",
        "rdm_train_path = f\"{notebook_path}/random_anime/\" # store randomized train anime\n",
        "\n",
        "test_path = f\"{notebook_path}/test_texts/\" # store parsed testing data\n",
        "rdm_test_path = f\"{notebook_path}/random_anime/\" # store randomized test anime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wCov-yF4BUS",
        "outputId": "b0eb61a0-9ef2-40ea-e1e6-e6015667000a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "''' \n",
        "Uncomment this cell to delete everything in a specific folder and its contents. Especially if all folders have been renamed! \n",
        "'''\n",
        "\n",
        "# import shutil\n",
        "# rm_folder = [\"train_texts\"]\n",
        "\n",
        "# print(\"removing folder(s)...\")\n",
        "# for f in rm_folder:\n",
        "#   shutil.rmtree(f\"{notebook_path}/{f}/\")\n",
        "\n",
        "# print(\"done!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' \\n\\nUncomment this cell to delete everything in a specific folder and its contents. Especiall if all folders have been renamed! \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBTlfDzDBhXR"
      },
      "source": [
        "\n",
        "---\n",
        "## ```I. Web Scraper```\n",
        "---\n",
        "\n",
        "First, we need to build a webscrapper for the kisunekko.net site!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AQcJlijeKmX"
      },
      "source": [
        "### 1.1. ```Use BeautifulSoup for Webscrapper```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YorlMf_EqhJ8",
        "outputId": "cff09ae7-4bd4-47ba-b378-92ac626e8798",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "'''\n",
        "Web-scraper for kistunekko.net\n",
        "'''\n",
        "\n",
        "domain = \"https://kitsunekko.net\"\n",
        "sub_query = \"/dirlist.php?dir=subtitles\"\n",
        "url = domain + sub_query\n",
        "res = requests.get(url)\n",
        "\n",
        "res"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Response [200]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCvK8S3e4r2X"
      },
      "source": [
        "soup = BeautifulSoup(res.content, 'html.parser')\n",
        "\n",
        "table_res = soup.find(id='flisttable') # id that points to the transcripts\n",
        "trans_elem = table_res.find_all('a', class_='') # Using the table results, retrieve the rows with links to transcripts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxVQKrNBBdWY"
      },
      "source": [
        "\n",
        "\n",
        "''' Strip html tags from text '''\n",
        "def clean_html(raw_html):\n",
        "  strip_tags = re.compile('<.*?>')\n",
        "  clean_text = re.sub(strip_tags, '', raw_html)\n",
        "\n",
        "  return clean_text\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pv2SBFNpCduX"
      },
      "source": [
        "'''\n",
        "Each Anime has a title and a link for download\n",
        "'''\n",
        "anime_list = {}\n",
        "for a_tag in trans_elem:\n",
        "    title_elem = a_tag.find('strong', class_='')\n",
        "    title = clean_html(str(title_elem))\n",
        "    anime_list[title] = a_tag[\"href\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mptd7FPXsIj1",
        "outputId": "e7e8266f-3800-43b1-ca3c-2afb7ff80891",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "'''Save list of anime for random generator.'''\n",
        "print(\"Writing to file...\")\n",
        "\n",
        "with open(anime_list_path, \"w+\") as f:\n",
        "  for anime in anime_list.keys():\n",
        "    f.write(anime)\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "f.close()\n",
        "\n",
        "print(\"Writing complete.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing to file...\n",
            "Writing complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAXMTKqRaKW8"
      },
      "source": [
        "### 1.2. ```Download compressed files from kisunekko.```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWDjT16OcO-X"
      },
      "source": [
        "'''\n",
        "Download file from kisunekko\n",
        "\n",
        "Dir: where to store the download\n",
        "URL: link to download the transcripts\n",
        "'''\n",
        "\n",
        "def download_files(url, dir):\n",
        "  zip_path = os.path.expanduser(dir)\n",
        "  download_to = zip_path + \"/\"\n",
        "\n",
        "  \n",
        "  if not os.path.exists(zip_path):\n",
        "      os.makedirs(zip_path)\n",
        "      try:\n",
        "        wget.download(url=url, out=download_to)\n",
        "\n",
        "      except:\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmOJ86UhVYjp"
      },
      "source": [
        "'''\n",
        "Get path to zip files for downloads.\n",
        "'''\n",
        "\n",
        "print(\"Downloading from Kistunekko.net...\")\n",
        "\n",
        "for zip_link in anime_list:\n",
        "  zip_title = zip_link\n",
        "  zip_url = domain + anime_list[zip_link]\n",
        "  zip_res = requests.get(zip_url)\n",
        "\n",
        "  soup = BeautifulSoup(zip_res.content, 'html.parser')\n",
        "\n",
        "  table_res = soup.find(id='flisttable')\n",
        "  trans_elem = table_res.find_all('a', class_='')\n",
        "\n",
        "  for a_tag in trans_elem:\n",
        "    trans_title = clean_html(str(zip_title))\n",
        "    download_url = domain + \"/\" + a_tag[\"href\"]\n",
        "    download_files(download_url, trans_path + trans_title)\n",
        "\n",
        "print(\"Download complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYyWVqWv-2fO"
      },
      "source": [
        "#### 1.2.1. Decompress and extract transcripts from zip files within a directory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqmvbAffuvrh"
      },
      "source": [
        "'''Check if directory exists. If not, create one. '''\n",
        "def check_path(file_path):\n",
        "  if not os.path.exists(file_path):\n",
        "    print(f\"creating dir: {file_path}\")\n",
        "    os.mkdir(file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoJZAZ8m9Kqk"
      },
      "source": [
        "def getListOfFiles(dir_path):\n",
        "\n",
        "    # create a list of file and sub directories \n",
        "    # names in the given directory \n",
        "    listOfFile = os.listdir(dir_path)\n",
        "    allFiles = list()\n",
        "\n",
        "    # Iterate over all the entries\n",
        "    for entry in listOfFile:\n",
        "\n",
        "        # Create full path\n",
        "        fullPath = os.path.join(dir_path, entry)\n",
        "\n",
        "        # If entry is a directory then get the list of files in this directory \n",
        "        if os.path.isdir(fullPath):\n",
        "            allFiles = allFiles + getListOfFiles(fullPath)\n",
        "        else:\n",
        "            allFiles.append(fullPath)\n",
        "                \n",
        "    return allFiles  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KryUHNRVdm9u"
      },
      "source": [
        "'''extract files from zip, rar, and .7zip files'''\n",
        "def decompress_files(trans_folder, trans_path):\n",
        "\n",
        "  for zip_file in trans_folder:\n",
        "    if ((\".rar\" in zip_file) or (\".zip\" in zip_file) or (\".7z\" in zip_file)):\n",
        "      save_to = os.path.splitext(zip_file)[0]\n",
        "      check_path(save_to)\n",
        "\n",
        "      with open(zip_file, \"rb\") as f:\n",
        "        try:\n",
        "          Archive(zip_file).extractall(save_to)\n",
        "          os.remove(zip_file) # remove zip file\n",
        "\n",
        "        except: # in case of a bad zip file or magic number error\n",
        "          pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSqIXdRtgay7"
      },
      "source": [
        "''' Gets a list of directories and subdirectories of the give path'''\n",
        "def return_path_list(file_path):\n",
        "  path_list = getListOfFiles(file_path)\n",
        "  path_list = list()\n",
        "\n",
        "  for (dirpath, dirnames, filenames) in os.walk(file_path):\n",
        "      path_list += [os.path.join(dirpath, file) for file in filenames]\n",
        "\n",
        "  return path_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPRynKDz9A7o",
        "outputId": "39d7e42b-06e4-4bf7-a1c8-b39314302aa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dir_list = return_path_list(trans_path)\n",
        "\n",
        "print(\"Decompressing files...\")\n",
        "decompress_files(dir_list, trans_path)\n",
        "print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decompressing files...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph1sk1KD8cYd"
      },
      "source": [
        "--------\n",
        "## ```II. Random Generator```\n",
        "--------\n",
        "\n",
        "This will be used to randomly select the anime we will train the model on!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdyA-czlkZ3m"
      },
      "source": [
        "### 2.1. ```Write/Read to File```\n",
        "Write or Read randomized anime list to or from a file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYclTogdknfZ"
      },
      "source": [
        "'''write the list of random anime to file'''\n",
        "def write_rdm_anime(rdm_list, rdm_path, file_name):\n",
        "  check_path(rdm_path)\n",
        "\n",
        "  with open(rdm_path + file_name, \"w+\") as f:\n",
        "    for anime in rdm_list:\n",
        "      f.write(anime)\n",
        "      f.write(\"\\n\")\n",
        "\n",
        "  f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Be7OErxolL9y"
      },
      "source": [
        "'''read the list of random anime to list'''\n",
        "def read_rdm_anime(rdm_path):\n",
        "  check_path(rdm_path)\n",
        "\n",
        "  rdm_list = []\n",
        "  with open(rdm_path, \"r\") as f:\n",
        "    rdm_list = [line.strip() for line in f]\n",
        "\n",
        "  f.close()\n",
        "\n",
        "  return rdm_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAJ53U_pmt9b"
      },
      "source": [
        "### 2.2. ```Random Generator```\n",
        "Generate N out of 2000 anime to train/test the model off of."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So1HAbYpv-JY"
      },
      "source": [
        "''' Read txt file into a list. Select 'N' random anime to be used for training.'''\n",
        "def list_random(n):\n",
        "    anime_list = read_rdm_anime(anime_list_path)\n",
        "  \n",
        "    random.shuffle(anime_list)\n",
        "    return anime_list[0:n]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPMZFzdvwnhn",
        "outputId": "917b4dff-2433-4717-ee8b-d89e52f1a365",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "rdm_anime_train = list_random(10)\n",
        "rdm_anime_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Grand Grix no Taka',\n",
              " 'Gaiking 2005',\n",
              " 'Concrete Revolutio: Choujin Gensou',\n",
              " 'Aki Sora',\n",
              " 'Bobobo-bo Bo-bobo',\n",
              " 'Dungeon ni Deai wo Motomeru no wa Machigatteiru Darou ka',\n",
              " 'Akame ga kill',\n",
              " 'Variable Geo',\n",
              " 'Radiant S1',\n",
              " 'Goblin Slayer']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sw7Cis0nob9",
        "outputId": "d50e5a9a-a653-4005-d1f9-84c8ef4e22e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "rdm_anime_test = list_random(10)\n",
        "rdm_anime_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Sailor Moon',\n",
              " 'Rokushin Gattai GodMars',\n",
              " 'Major',\n",
              " 'Tsugumomo',\n",
              " 'Isekai Quartet',\n",
              " 'Legend of the Legendaries Heroes',\n",
              " 'Ikki Tousen Dragon Destiny',\n",
              " 'Barakamon',\n",
              " 'Hana no Ko Lun Lun',\n",
              " 'Maoyuu Maou Yuusha']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nC-XVO8nUqb"
      },
      "source": [
        "print(\"writing to file...\")\n",
        "write_rdm_anime(rdm_anime_train, rdm_train_path, \"/random_train.txt\")\n",
        "write_rdm_anime(rdm_anime_test, rdm_test_path, \"/random_test.txt\")\n",
        "print(\"done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alql3gUPxJXG"
      },
      "source": [
        "---\n",
        "## ```III. Data Preparation```\n",
        "---\n",
        "Prep training data for the model using GloVe\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJmcEm3n8wNt"
      },
      "source": [
        "### 3.1. ```Clean Transcripts```\n",
        "\n",
        "After randomly selecting the anime, we will select 10 episodes from each anime and run it through the pysubparser."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd9SvOdDEwC3"
      },
      "source": [
        "#### 3.1.1. File Handling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2R3UzV4pJyI"
      },
      "source": [
        "'''\n",
        "Save text to text file.\n",
        "'''\n",
        "def save_file(text_path, text, save_to):\n",
        "  check_path(text_path)\n",
        "\n",
        "  with open(save_to, \"w+\") as f:\n",
        "    for line in text:\n",
        "      f.write(line)\n",
        "\n",
        "  f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aozAwOEgwUw"
      },
      "source": [
        "''' \n",
        "Write transcript text (paragraph) onto a text file. Path depends on if it's to\n",
        "be used for training data or testing data.\n",
        "'''\n",
        "def text_to_file(sub_file, text, text_path):\n",
        "  base_name = os.path.basename(sub_file)\n",
        "  file_name = os.path.splitext(base_name)[0]\n",
        "\n",
        "  save_to = text_path + file_name + \".txt\"\n",
        "  save_file(text_path, text, save_to)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Qq2BRBrE0id"
      },
      "source": [
        "#### 3.1.2. Subtitle Parser\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1PJfO1f8PzI"
      },
      "source": [
        "'''\n",
        "Use pysubparser to get parse text from subtitles!\n",
        "\n",
        "As of Nov 06: \n",
        "  Fix:\n",
        "  (1) extract episode number OR name from file metadata -- let's stay consistent\n",
        "'''\n",
        "\n",
        "def parse_subtitle(sub_file, text_path, anime_title):\n",
        "  text = ''\n",
        "  text += anime_title + \"\\n\"\n",
        "\n",
        "  if \".ass\" in sub_file or \".srt\" in sub_file or \".ssa\" in sub_file or \".sub\" in sub_file or \".txt\" in sub_file:\n",
        "    subtitles = parser.parse(sub_file)\n",
        "\n",
        "    # convert subtitltes to lowercase\n",
        "    lower_sub = brackets.clean(\n",
        "        lower_case.clean(\n",
        "            subtitles\n",
        "        )\n",
        "    )\n",
        "\n",
        "    for subtitle in lower_sub:\n",
        "      text += subtitle.text + \" \"\n",
        "      \n",
        "    text_to_file(sub_file, text, text_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kg3Pfc_YajR",
        "outputId": "bbc93469-ba72-4d19-fac2-6742099aebda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "'''\n",
        "Given a list of random anime, return all paths and parse the subtitles into training text file.\n",
        "'''\n",
        "\n",
        "train_anime_list = read_rdm_anime(rdm_train_path + \"/random_train.txt\")\n",
        "\n",
        "print(\"parsing training subtitles...\")\n",
        "for anime in train_anime_list:\n",
        "  anime_dir = trans_path + anime\n",
        "  subtitle_path = return_path_list(anime_dir)\n",
        "\n",
        "  for sub_file in subtitle_path:\n",
        "    parse_subtitle(sub_file, train_path, anime)\n",
        "\n",
        "print(\"done!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "parsing training subtitles...\n",
            "done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSTAK6xf1OqO"
      },
      "source": [
        "### 3.2. ```Glove Model```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "260T_-rW4WvQ"
      },
      "source": [
        "#### 3.2.1. Download GloVe\n",
        "If not already installed, download here!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFwlYyQPt8YU"
      },
      "source": [
        "''' download the glove file from nlp.stanford'''\n",
        "\n",
        "glove_url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "download_to = '/content/drive/My Drive/datasets/glove/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38gZrT9a5_gi"
      },
      "source": [
        "check_path(download_to)\n",
        "\n",
        "print(\"downloading glove dataset\")\n",
        "wget.download(url=glove_url, out=download_to)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhykXmLjw32A"
      },
      "source": [
        "''' decompress gloVe zip file'''\n",
        "zip_ref = zipfile.ZipFile(f'{download_to}/glove.6B.zip', 'r')\n",
        "zip_ref.extractall(f'{download_to}/glove.6B/')\n",
        "zip_ref.close()\n",
        "print(\"done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVwWVXTu4cOs"
      },
      "source": [
        "#### 3.2.2. Build GloVe Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdGBBPeSz9--"
      },
      "source": [
        "'''\n",
        "Load GloVe model with word embeddings.\n",
        "'''\n",
        "def loadGloveModel(file): # from Karishma Malkan on stackoverflow\n",
        "  print(\"Loading Glove Model\")\n",
        "\n",
        "  f = open(file,'r', encoding=\"utf-8\") \n",
        "  model = {}\n",
        "  for line in f:\n",
        "    splitLine = line.split()\n",
        "    word = splitLine[0]\n",
        "    embedding = np.array([float(val) for val in splitLine[1:]])\n",
        "    model[word] = embedding\n",
        "    \n",
        "  print(\"Done.\",len(model),\" words loaded!\") \n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oX8hypnC0rL9"
      },
      "source": [
        "'''\n",
        "Use euclidian distance to find words associated with target word.\n",
        "'''\n",
        "def find_similarities(embedding, model):\n",
        "  return sorted(model.keys(), key=lambda word: spatial.distance.euclidean(model[word], embedding))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bO-LHZzfzfFs",
        "outputId": "514b540a-5453-44d8-b2a9-791f34f82aff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "''' Build GloVe model '''\n",
        "embed_model = loadGloveModel(f'{download_to}/glove.6B/glove.6B.50d.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Glove Model\n",
            "Done. 400000  words loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9xn9WAbnm-I"
      },
      "source": [
        "#### 3.2.3. Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05Rz4euAOZW4"
      },
      "source": [
        "''' \n",
        "Remove and add words to list of words. Helpful is there are some words you\n",
        "don't want associated with a label. \n",
        "'''\n",
        "def edit_words(tag, rem_list, add_list):\n",
        "\n",
        "  for w in rem_list:\n",
        "    while w in tag: tag.remove(w) \n",
        "\n",
        "  for w in add_list:\n",
        "    if w not in tag:\n",
        "      tag.append(w)\n",
        "\n",
        "  # remove duplicates\n",
        "  tag = list(dict.fromkeys(tag) )\n",
        "  \n",
        "  return tag\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irXSjnGbwusA"
      },
      "source": [
        "''' \n",
        "Use word similarities to find words associated with our labels.\n",
        "There is also a list of words you can remove or add.\n",
        "'''\n",
        "## Romance\n",
        "romance_list = find_similarities(embed_model['romance'], embed_model)[0:20]\n",
        "marriage_list = find_similarities(embed_model['divorce'], embed_model)[0:20]\n",
        "romance = romance_list + marriage_list\n",
        "\n",
        "rom_rem = [\"fantasy\", \n",
        "           \"melodrama\", \n",
        "           \"obsession\", \n",
        "           \"retelling\",\n",
        "           \"marriages\",\n",
        "           \"revolves\",\n",
        "           \"fantasies\",\n",
        "           \"explores\",\n",
        "           \"mystery\"\n",
        "          \"novel\",\n",
        "          \"adventures\",\n",
        "           \"fascination\",\n",
        "           \"fable\",\n",
        "            \"heroine\",\n",
        "           \"lapsed\",\n",
        "           \"romances\",\n",
        "           \"divorces\",\n",
        "           \"marital\",\n",
        "           \"consent\"]\n",
        "\n",
        "rom_add = [\"kiss\", \n",
        "          \"lover\", \n",
        "           \"confess\", \n",
        "           \"confession\", \n",
        "           \"engagement\", \n",
        "           \"engaged\", \n",
        "           \"fiance\", \n",
        "           \"fiancee\", \n",
        "           \"boyfriend\", \n",
        "           \"girlfriend\"]\n",
        "\n",
        "romance_tag = edit_words(romance, rom_rem, rom_add)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJc-iCE4886v"
      },
      "source": [
        "## Supernatural\n",
        "magic_list = find_similarities(embed_model['magical'], embed_model)[0:20]\n",
        "creatures_list = find_similarities(embed_model['vampire'], embed_model)[0:20]\n",
        "supernatural = magic_list + creatures_list\n",
        "\n",
        "super_rem = [\"marvelous\", \n",
        "             \"wondrous\", \n",
        "             \"cinematic\", \n",
        "             \"imagination\", \n",
        "             \"essence\", \n",
        "             \"protagonist\", \n",
        "             \"villain\", \n",
        "             \"rabbit\", \n",
        "             \"spider\", \n",
        "             \"inspiration\", \n",
        "             \"fantastic\", \n",
        "             \"sorts\"]\n",
        "\n",
        "super_add = [\"psychic\", \n",
        "             \"cursed\", \n",
        "             \"spirit\", \n",
        "             \"ghost\", \n",
        "             \"haunted\", \n",
        "             \"zombie\", \n",
        "             \"demon\", \n",
        "             \"monk\"]\n",
        "\n",
        "supernatural_tag = edit_words(supernatural, super_rem, super_add)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jS0y_r2C89PL"
      },
      "source": [
        "## Death\n",
        "death_list = find_similarities(embed_model['death'], embed_model)[0:20]\n",
        "murder_list = find_similarities(embed_model['murder'], embed_model)[0:20]\n",
        "death = death_list + murder_list\n",
        "\n",
        "death_rem = [\"taken\", \n",
        "             \"another\", \n",
        "             \"brought\", \n",
        "             \"father\", \n",
        "             \"was\", \n",
        "             \"birth\"]\n",
        "\n",
        "death_add = [\"funeral\", \n",
        "             \"criminal\", \n",
        "             \"arrest\", \n",
        "             \"abduction\"]\n",
        "\n",
        "death_tag = edit_words(death, death_rem, death_add)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ijyNO7iPd_1",
        "outputId": "25a59c64-d887-436b-b545-66139d541f91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# length of label vectors used are the same\n",
        "print(f\"romance: {len(romance_tag)} \\nsupernatural: {len(supernatural_tag)} \\ndeath: {len(death_tag)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "romance: 33 \n",
            "supernatural: 33 \n",
            "death: 33\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNF_UiokoPMn"
      },
      "source": [
        "'''\n",
        "If an word from one of the below label vectors (death, supernatural, romance) \n",
        "is found in unique_words[], tag it.\n",
        "\n",
        "Nov 7\n",
        "\n",
        "Suggestion:\n",
        "(1) Can we look for variations of a word by stemming the vectors?\n",
        "'''\n",
        "def check_death(unique_words):\n",
        "  for w in unique_words:\n",
        "    if w in death_tag:\n",
        "      return True\n",
        "      break\n",
        "\n",
        "def check_supernatural(unique_words):\n",
        "  for w in unique_words:\n",
        "    if w in supernatural_tag:\n",
        "      return True\n",
        "      break\n",
        "\n",
        "def check_romance(unique_words):\n",
        "  for w in unique_words:\n",
        "    if w in romance_tag:\n",
        "      return True\n",
        "      break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mJBFg-THX5J"
      },
      "source": [
        "#### 3.2.4. Transcript Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLSUf1JCPOVM"
      },
      "source": [
        "'''Normalize transcript '''\n",
        "def normalize_document(doc):\n",
        "    wpt = nltk.WordPunctTokenizer()\n",
        "    stop_words = nltk.corpus.stopwords.words('english')\n",
        "    \n",
        "    # lower case and remove special characters and whitespaces\n",
        "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
        "    doc = doc.lower()\n",
        "    doc = doc.strip()\n",
        "\n",
        "    # tokenize document\n",
        "    tokens = wpt.tokenize(doc)\n",
        "\n",
        "    # filter out stopwords from document\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # re-create document from filtered tokens\n",
        "    doc = ' '.join(filtered_tokens)\n",
        "    \n",
        "    return doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTvhgYU3txaX"
      },
      "source": [
        "'''\n",
        "Read text from a file to a list.\n",
        "'''\n",
        "def read_text(text_file):\n",
        "  transcript = []\n",
        "  title = \"\"\n",
        "  base_name = os.path.basename(text_file)\n",
        "  episode_name = os.path.splitext(base_name)[0]\n",
        "  \n",
        "  with open(text_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "    title = f.readline()\n",
        "\n",
        "    for line in f:\n",
        "      c_line = re.sub(r'\\{(.*?)\\}', '', line, re.I|re.A)\n",
        "      c_line = c_line.lower()\n",
        "      c_line = c_line.strip()\n",
        "      transcript.append(c_line.strip())\n",
        "\n",
        "\n",
        "  return np.array(transcript), title, episode_name  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxp9Ybng02bA",
        "outputId": "b779636f-baf7-48d9-80d7-3332df6bc358",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "'''\n",
        "For each text file in the training path, save the transcript, anime information,\n",
        "and labels to later be used for the model.\n",
        "'''\n",
        "\n",
        "text_list = return_path_list(train_path)\n",
        "\n",
        "pd_trans = [] # transcripts\n",
        "pd_title = [] # anime title\n",
        "pd_epname = [] # episode name\n",
        "\n",
        "pd_rom = [] # romance\n",
        "pd_supernat = [] # supernatural\n",
        "pd_death = [] # death\n",
        "\n",
        "''' Read content from each text file and categorize it '''\n",
        "print(\"reading to file....\")\n",
        "for text_file in text_list:\n",
        "  transcript, title, epname = read_text(text_file)\n",
        "  pd_trans.append(transcript)\n",
        "  pd_title.append(title.strip().split('\\n'))\n",
        "  pd_epname.append(epname)\n",
        "\n",
        "  # normalize transcript\n",
        "  vec_transcript = np.vectorize(normalize_document)\n",
        "  norm_transcript = vec_transcript(transcript)\n",
        "\n",
        "  # get unique words from the transcript\n",
        "  unique_words = list(set([word for sublist in [trans.split() for trans in norm_transcript] for word in sublist]))\n",
        "\n",
        "  # check if category appears in the transcript\n",
        "  has_death = check_death(unique_words)\n",
        "  pd_death.append(has_death)\n",
        "\n",
        "  has_supernat = check_supernatural(unique_words)\n",
        "  pd_supernat.append(has_supernat)\n",
        "\n",
        "  has_rom = check_romance(unique_words)\n",
        "  pd_rom.append(has_rom)\n",
        "\n",
        "print(\"done!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading to file....\n",
            "done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ny8CPhc6Iok_"
      },
      "source": [
        "### 3.3. ```Convert Dataset to DataFrame```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyMAz1Fsp8KX"
      },
      "source": [
        "''' \n",
        "Convert dataset to dataframe for further manipulation and better visualization.\n",
        "\n",
        "Nov 7\n",
        "Fix:\n",
        "(1) replace episode name with episode # (if necessary)\n",
        "\n",
        " '''\n",
        "def create_training_df(ep_name, anime_title, anime_transcript, death_label, supernat_label, rom_label):\n",
        "    df = pd.DataFrame({'text': anime_transcript, \n",
        "                          'anime_title': anime_title,\n",
        "                          'death': death_label,\n",
        "                          'supernatural': supernat_label,\n",
        "                          'romance': rom_label,\n",
        "                          'episode_title': ep_name})\n",
        "\n",
        "    anime_df = df[['text', 'anime_title', 'death','supernatural', 'romance', \"episode_title\"]]\n",
        "\n",
        "    df = convert_labels(anime_df)\n",
        "\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gr7jeog_GlBT"
      },
      "source": [
        "''' Replace True with 1 and False with 0 '''\n",
        "def convert_labels(df):\n",
        "  df['death'] = [1 if x == True else 0 for x in df['death']]\n",
        "  df['supernatural'] = [1 if x == True else 0 for x in df['supernatural']]\n",
        "  df['romance'] = [1 if x == True else 0 for x in df['romance']]\n",
        "\n",
        "  return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WL86rtD5FAT",
        "outputId": "917af903-d18f-4c40-bff0-f6602d199221",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "'''\n",
        "Create dataframe for training data and save as csv.\n",
        "'''\n",
        "\n",
        "anime_df = create_training_df(pd_epname, pd_title, pd_trans, pd_death, pd_supernat, pd_rom) \n",
        "anime_df.to_csv(f\"{notebook_path}/train_anime.csv\", index=False)\n",
        "anime_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>anime_title</th>\n",
              "      <th>death</th>\n",
              "      <th>supernatural</th>\n",
              "      <th>romance</th>\n",
              "      <th>episode_title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[gaiking gai gai gai daiku maryu gaiking gai g...</td>\n",
              "      <td>[Gaiking 2005]</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Gaiking Legend of Daiku-Maryu - 39_track3_eng</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[gaiking gaiking gai gai gai daiku maryu gaiki...</td>\n",
              "      <td>[Gaiking 2005]</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Gaiking Legend of Daiku-Maryu - 01_track3_eng</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[gaiking gaiking gai gai gai daiku maryu gaiki...</td>\n",
              "      <td>[Gaiking 2005]</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Gaiking Legend of Daiku-Maryu - 02_track3_eng</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[gaiking gaiking gai gai gai daiku maryu gaiki...</td>\n",
              "      <td>[Gaiking 2005]</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Gaiking Legend of Daiku-Maryu - 03_track3_eng</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[gaiking gaiking gai gai gai daiku maryu gaiki...</td>\n",
              "      <td>[Gaiking 2005]</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Gaiking Legend of Daiku-Maryu - 04_track3_eng</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>[master, what should i do today? what should y...</td>\n",
              "      <td>[Goblin Slayer]</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>[Moozzi2] Goblin Slayer - 08 (BD 1920x1080 x.2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>[there and back again now, then... what's rath...</td>\n",
              "      <td>[Goblin Slayer]</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>[Moozzi2] Goblin Slayer - 09 (BD 1920x1080 x.2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>[when i was a child, i thought i'd become an a...</td>\n",
              "      <td>[Goblin Slayer]</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>[Moozzi2] Goblin Slayer - 10 (BD 1920x1080 x.2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>[a gathering of adventurers good morning! just...</td>\n",
              "      <td>[Goblin Slayer]</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>[Moozzi2] Goblin Slayer - 11 (BD 1920x1080 x.2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>[how did it come to this? that horde is finish...</td>\n",
              "      <td>[Goblin Slayer]</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>[Moozzi2] Goblin Slayer - 12 END (BD 1920x1080...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>125 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  text  ...                                      episode_title\n",
              "0    [gaiking gai gai gai daiku maryu gaiking gai g...  ...      Gaiking Legend of Daiku-Maryu - 39_track3_eng\n",
              "1    [gaiking gaiking gai gai gai daiku maryu gaiki...  ...      Gaiking Legend of Daiku-Maryu - 01_track3_eng\n",
              "2    [gaiking gaiking gai gai gai daiku maryu gaiki...  ...      Gaiking Legend of Daiku-Maryu - 02_track3_eng\n",
              "3    [gaiking gaiking gai gai gai daiku maryu gaiki...  ...      Gaiking Legend of Daiku-Maryu - 03_track3_eng\n",
              "4    [gaiking gaiking gai gai gai daiku maryu gaiki...  ...      Gaiking Legend of Daiku-Maryu - 04_track3_eng\n",
              "..                                                 ...  ...                                                ...\n",
              "120  [master, what should i do today? what should y...  ...  [Moozzi2] Goblin Slayer - 08 (BD 1920x1080 x.2...\n",
              "121  [there and back again now, then... what's rath...  ...  [Moozzi2] Goblin Slayer - 09 (BD 1920x1080 x.2...\n",
              "122  [when i was a child, i thought i'd become an a...  ...  [Moozzi2] Goblin Slayer - 10 (BD 1920x1080 x.2...\n",
              "123  [a gathering of adventurers good morning! just...  ...  [Moozzi2] Goblin Slayer - 11 (BD 1920x1080 x.2...\n",
              "124  [how did it come to this? that horde is finish...  ...  [Moozzi2] Goblin Slayer - 12 END (BD 1920x1080...\n",
              "\n",
              "[125 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7vwsFYDO5kL"
      },
      "source": [
        "---\n",
        "## ```IV. Build Model```\n",
        "---\n",
        "\n",
        "The model we will be using is K-Nearest Neighbors.\n",
        "\n",
        "[explaination why here]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoA9X9uy-HTq"
      },
      "source": [
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        "\n",
        "# Models\n",
        "from skmultilearn.adapt import MLkNN # K-Nearest Neighbors\n",
        "from skmultilearn.problem_transform import BinaryRelevance # Binary Relevance\n",
        "from sklearn.naive_bayes import GaussianNB # Gaussian Naive Bayes\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.metrics import accuracy_score "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eshE3mGzlmmO"
      },
      "source": [
        "### 4.1. ```Train Model```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oguzcyyET03L",
        "outputId": "3d04d6bb-1971-4990-c6b9-c2b96d34d860",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "'''\n",
        "Read from CSV and vectorize dataset for training the model.\n",
        "'''\n",
        "\n",
        "anime_df = pd.read_csv(f'{notebook_path}/train_anime.csv') \n",
        "X = anime_df[\"text\"] \n",
        "y = np.asarray(anime_df[[\"death\",\"supernatural\", \"romance\"]]) \n",
        "  \n",
        "# initializing TfidfVectorizer  \n",
        "vect_tf = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,2), max_df=0.85)\n",
        "\n",
        "# fitting the tf-idf on the given data \n",
        "vect_tf.fit(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
              "                input='content', lowercase=True, max_df=0.85, max_features=None,\n",
              "                min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\n",
              "                smooth_idf=True, stop_words=None, strip_accents='unicode',\n",
              "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, use_idf=True, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbN56HNo3GP2"
      },
      "source": [
        "'''\n",
        "Train model using vectorized datasets.\n",
        "'''\n",
        "\n",
        "# split the data into train and test sets \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, shuffle=True) \n",
        "  \n",
        "# transform datasets\n",
        "vect_Xtrain = vect_tf.transform(X_train) \n",
        "vect_Xtest = vect_tf.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N9wQueY3oIU"
      },
      "source": [
        "#### 4.1.1. Compare Models\n",
        "We will choose the best model to use by comparing the following models' accuracy.\n",
        "\n",
        "1. MLkNN Model\n",
        "2. Naiive Bayes Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMB-PF7xlJZZ"
      },
      "source": [
        "threshold = 0.3 # threshold value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqFBgtqhO3kq",
        "outputId": "69a9ce82-46bc-4641-ab59-f8d4bfa11815",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "'''\n",
        "Use MLkNN model for multilabel classification\n",
        "'''\n",
        "mlknn_classifier = MLkNN()\n",
        "mlknn_classifier.fit(vect_Xtrain, y_train)\n",
        "\n",
        "y_pred = mlknn_classifier.predict(vect_Xtest)\n",
        "  \n",
        "# accuracy\n",
        "y_thres = (y_pred >= threshold).astype(int)\n",
        "\n",
        "print(f\"K-Nearest Neighbors Model \\n ------------\")\n",
        "print(f\"f1-score: {f1_score(y_test, y_pred, average='micro')}\")\n",
        "print(f\"f1-score threshold: {f1_score(y_test, y_thres, average='micro')}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "K-Nearest Neighbors Model \n",
            " ------------\n",
            "f1-score: 0.9\n",
            "f1-score threshold: 0.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjNksWi0wpuZ",
        "outputId": "025cc7a7-ddea-4512-a45f-39f987b01843",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "'''\n",
        "Use Naive Bayes for multilabel classification\n",
        "'''\n",
        "\n",
        "# initialize binary relevance multi-label classifier\n",
        "# with a gaussian naive bayes base classifier\n",
        "bayes = BinaryRelevance(GaussianNB())\n",
        "\n",
        "# train\n",
        "bayes.fit(vect_Xtrain, y_train)\n",
        "\n",
        "# predict\n",
        "y_pred = bayes.predict(vect_Xtest) \n",
        "\n",
        "# accuracy\n",
        "y_thres = (y_pred >= threshold).astype(int) # threshold\n",
        "\n",
        "print(f\"Naiive Bayes Model \\n ------------\")\n",
        "print(f\"f1-score: {f1_score(y_test, y_pred, average='micro')}\")\n",
        "print(f\"f1-score threshold: {f1_score(y_test, y_thres, average='micro')}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naiive Bayes Model \n",
            " ------------\n",
            "f1-score: 0.8734177215189874\n",
            "f1-score threshold: 0.8734177215189874\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjSP4OcNlrDD"
      },
      "source": [
        "### 4.2. ```Test Model Predictions```\n",
        "Whatever we do to the training data, we do the same to the testing data!\n",
        "\n",
        "\n",
        "__Nov 6 Note__: We need more data to improve the model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZ6rR3xaVovl",
        "outputId": "dd339c31-4e5c-4d2f-f767-b8451915c670",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "'''\n",
        "Given a list of random anime, return all paths and parse the subtitles into test text file.\n",
        "'''\n",
        "\n",
        "test_anime_list = read_rdm_anime(rdm_test_path + \"/random_test.txt\")\n",
        "\n",
        "print(\"\\nparsing test subtitles...\")\n",
        "for anime in test_anime_list:\n",
        "  anime_dir = trans_path + anime\n",
        "  subtitle_path = return_path_list(anime_dir)\n",
        "\n",
        "  for sub_file in subtitle_path:\n",
        "    parse_subtitle(sub_file, test_path, anime)\n",
        "\n",
        "print(\"done!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "parsing test subtitles...\n",
            "done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlNXU7sPXz-E",
        "outputId": "3e1eb535-57c7-4f82-eac4-6c1abc5cfb84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# glimpse at the list of anime\n",
        "test_anime_list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Sailor Moon',\n",
              " 'Rokushin Gattai GodMars',\n",
              " 'Major',\n",
              " 'Tsugumomo',\n",
              " 'Isekai Quartet',\n",
              " 'Legend of the Legendaries Heroes',\n",
              " 'Ikki Tousen Dragon Destiny',\n",
              " 'Barakamon',\n",
              " 'Hana no Ko Lun Lun',\n",
              " 'Maoyuu Maou Yuusha']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USfVf3BrW_NC"
      },
      "source": [
        "'''\n",
        "Get path of test data texts and append each text to a list of transcripts.\n",
        "'''\n",
        "test_list = return_path_list(test_path)\n",
        "\n",
        "pd_test_trans = []\n",
        "pd_test_title = []\n",
        "pd_test_epname = []\n",
        "\n",
        "''' Read content from each text file and categorize it '''\n",
        "for text_file in test_list:\n",
        "  transcript, title, epname = read_text(text_file)\n",
        "  pd_test_trans.append(transcript)\n",
        "  pd_test_title.append(title.strip().split('\\n'))\n",
        "  pd_test_epname.append(epname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jWQVp70TlIX",
        "outputId": "e1609f8a-fdbb-4ead-b959-fd8346116998",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "'''\n",
        "Convert list of texts to dataframe, along with episode name and labels.\n",
        "'''\n",
        "df = pd.DataFrame({'text': pd_test_trans,\n",
        "                        'anime_title': pd_test_title,\n",
        "                   'episode_title': pd_test_epname})\n",
        "\n",
        "test_df = df[['text', 'anime_title', 'episode_title']]\n",
        "\n",
        "text_df = test_df[['text']]\n",
        "vect_test = vect_tf.transform(text_df) \n",
        "  \n",
        "pred_labels = mlknn_classifier.predict(vect_test)\n",
        "pred_labels = pred_labels.toarray()\n",
        "\n",
        "pred_labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSUI-VZMbs0D",
        "outputId": "e1235e2c-e834-4017-f40c-be8494a7a896",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "'''\n",
        "save preditions to dataframe and print results\n",
        "'''\n",
        "pred_death = [row[0] for row in pred_labels]\n",
        "pred_supernat = [row[1] for row in pred_labels]\n",
        "pred_romance = [row[2] for row in pred_labels]\n",
        "\n",
        "label_df = pd.DataFrame({\"Death\": pred_death,\n",
        "                        \"Supernatural\": pred_supernat,\n",
        "                        \"Romance\": pred_romance})\n",
        "\n",
        "# join anime information with their predicted labels\n",
        "pred_df = pd.concat([test_df, label_df], axis=1)\n",
        "\n",
        "pred_df.to_csv(f\"{notebook_path}/test_anime.csv\", index=False)\n",
        "pred_csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>anime_title</th>\n",
              "      <th>episode_title</th>\n",
              "      <th>Death</th>\n",
              "      <th>Supernatural</th>\n",
              "      <th>Romance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>['subtitles by explosiveskull www.opensubtitle...</td>\n",
              "      <td>['Major']</td>\n",
              "      <td>Batman.Death.in.the.Family.2020.REPACK.720p.Bl...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>['when the obi, wound up like a cocoon, unfurl...</td>\n",
              "      <td>['Tsugumomo']</td>\n",
              "      <td>[Moozzi2] Tsugumomo - 01 (BD 1920x1080 x.264 F...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>['kazuya! come, it is morning. honestly, sleep...</td>\n",
              "      <td>['Tsugumomo']</td>\n",
              "      <td>[Moozzi2] Tsugumomo - 02 (BD 1920x1080 x.264 F...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>['you there. th... they\\'re huge! you are kaga...</td>\n",
              "      <td>['Tsugumomo']</td>\n",
              "      <td>[Moozzi2] Tsugumomo - 03 (BD 1920x1080 x.264 F...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>['water impact. oh, no. oho. how\\'s that, kuku...</td>\n",
              "      <td>['Tsugumomo']</td>\n",
              "      <td>[Moozzi2] Tsugumomo - 04 (BD 1920x1080 x.264 F...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>[\"i-i... i-i've brought you some tea! thank yo...</td>\n",
              "      <td>['Maoyuu Maou Yuusha']</td>\n",
              "      <td>[Moozzi2] Maoyuu Maou Yuusha - 08 (BD 1920x108...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>['the central nations have made their move, it...</td>\n",
              "      <td>['Maoyuu Maou Yuusha']</td>\n",
              "      <td>[Moozzi2] Maoyuu Maou Yuusha - 09 (BD 1920x108...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>['to summarize, by accepting her speech, we\\'v...</td>\n",
              "      <td>['Maoyuu Maou Yuusha']</td>\n",
              "      <td>[Moozzi2] Maoyuu Maou Yuusha - 10 (BD 1920x108...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>['there\\'s a 3 point increase! buy now, as muc...</td>\n",
              "      <td>['Maoyuu Maou Yuusha']</td>\n",
              "      <td>[Moozzi2] Maoyuu Maou Yuusha - 11 (BD 1920x108...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>['mage! so sleepy... this is one of the legend...</td>\n",
              "      <td>['Maoyuu Maou Yuusha']</td>\n",
              "      <td>[Moozzi2] Maoyuu Maou Yuusha - 12 END (BD 1920...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>74 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 text  ... Romance\n",
              "0   ['subtitles by explosiveskull www.opensubtitle...  ...     0.0\n",
              "1   ['when the obi, wound up like a cocoon, unfurl...  ...     0.0\n",
              "2   ['kazuya! come, it is morning. honestly, sleep...  ...     0.0\n",
              "3   ['you there. th... they\\'re huge! you are kaga...  ...     NaN\n",
              "4   ['water impact. oh, no. oho. how\\'s that, kuku...  ...     NaN\n",
              "..                                                ...  ...     ...\n",
              "69  [\"i-i... i-i've brought you some tea! thank yo...  ...     NaN\n",
              "70  ['the central nations have made their move, it...  ...     NaN\n",
              "71  ['to summarize, by accepting her speech, we\\'v...  ...     NaN\n",
              "72  ['there\\'s a 3 point increase! buy now, as muc...  ...     NaN\n",
              "73  ['mage! so sleepy... this is one of the legend...  ...     NaN\n",
              "\n",
              "[74 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-sfsVvq56ev"
      },
      "source": [
        "--End--"
      ]
    }
  ]
}